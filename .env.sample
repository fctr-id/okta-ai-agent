# ===================================================================
#  Application Configuration
# ===================================================================
#
# Please fill out the sections below. Start with the required Okta
# and AI Provider settings, then review the optional settings.
#

# -------------------------------------------------------------------
#  1. Core Okta & Security Configuration (Required)
# -------------------------------------------------------------------

# Your Okta organization URL.
OKTA_CLIENT_ORGURL=https://dev-2715.okta.com

# An API token generated from the Okta Admin Console.
# A read-only admin role is sufficient.
OKTA_API_TOKEN=

## Session and Security
# How long a user's session cookie should last. Set the value in minutes and ensure both this and JWT_ACCESS_TOKEN_EXPIRE_MINUTES are the same.
COOKIE_SECURE=True
COOKIE_MAX_AGE_MINUTES=30
JWT_ACCESS_TOKEN_EXPIRE_MINUTES=30


# -------------------------------------------------------------------
#  2. Okta User Sync Configuration (Optional)
# -------------------------------------------------------------------

# Specify additional user attributes from Okta to sync into the database. 
# the current default attributes are: https://github.com/fctr-id/okta-ai-agent?tab=readme-ov-file#database-mode-data-model
#
# Example: OKTA_USER_CUSTOM_ATTRIBUTES=costCenter,division,hireDate
OKTA_USER_CUSTOM_ATTRIBUTES=

# ---------------------------------------------------------------
# Deprovisioned User Sync Configuration (Optional)
# ---------------------------------------------------------------
# By default, Okta syncs all active users but EXCLUDES deprovisioned/disabled users.
# Use these settings to also sync deprovisioned users with specific criteria:

# Include deprovisioned/disabled users in the sync (true/false)
# Set to 'false' to exclude all deprovisioned users (default Okta behavior)
SYNC_DEPROVISIONED_USERS=true

# Only sync deprovisioned users created after this date (YYYY-MM-DD format)
# If blank, creation date will NOT be used as filter criteria for deprovisioned users
# Example: DEPR_USER_CREATED_AFTER=2024-01-01
DEPR_USER_CREATED_AFTER=

# Only sync deprovisioned users updated after this date (YYYY-MM-DD format)  
# If blank, last updated date will NOT be used as filter criteria for deprovisioned users
# Example: DEPR_USER_UPDATED_AFTER=2025-03-01
DEPR_USER_UPDATED_AFTER=2025-03-01

# BEHAVIOR SUMMARY:
# - SYNC_DEPROVISIONED_USERS=false: No deprovisioned users synced
# - SYNC_DEPROVISIONED_USERS=true + both dates blank: ALL deprovisioned users synced  
# - SYNC_DEPROVISIONED_USERS=true + dates provided: Only deprovisioned users meeting date criteria synced


# ---------------------------------------------------------------
# Device Sync (Optional - Defaults to false)
# ---------------------------------------------------------------
# Set to 'true' to sync Okta device information and user-device relationships. 
# This will take longer and require more API calls.
SYNC_OKTA_DEVICES=false

# -------------------------------------------------------------------
#  3. Performance Tuning (REQUIRED and Important)
# -------------------------------------------------------------------

# --- Okta Concurrent Speed (OKTA_CONCURRENT_LIMIT) ---
# This setting controls how many requests the app sends to Okta at the same time.
# Setting this correctly is key to a fast and successful data sync.
#
# Follow these steps to find the correct value for your setup:
#
# 1. WHAT TYPE OF OKTA ACCOUNT DO YOU HAVE?
#    - A free "Developer" account, or a paid "Enterprise" account?
#
# 2. WHAT IS YOUR API TOKEN'S RATE LIMIT?
#    - In your Okta Admin Console, go to: Security > API > Tokens.
#    - Click the token name you are using for this app and check its "Rate limit across all APIs" percentage limit.
#
# 3. SET THE CONCURRENT SETTING VALUE BELOW BASED ON YOUR RATE LIMIT
# - EXTREMELY IMPORTANT: Refer to this table in the README for guidance to set the OKTA_CONCURRENT_LIMIT: https://github.com/fctr-id/okta-ai-agent?tab=readme-ov-file#-optimal-api-settings-for-sync-
#
OKTA_CONCURRENT_LIMIT=15


# -------------------------------------------------------------------
#  4. AI Provider Selection (Required)
# -------------------------------------------------------------------
# First, choose your AI provider here.
# Then, ONLY fill in the corresponding section for that provider below.
#
# Options: vertex_ai, openai, azure_openai, anthropic, openai_compatible, bedrock
AI_PROVIDER=openai


# =========== AI Provider Specific Settings ==========
#   Fill out ONLY the section that matches your AI_PROVIDER choice above
# =====================================================

# --- Vertex AI Settings ---
# Note: gemini-1.5-pro for coding and gemini-1.5-flash for reasoning often work best.
# Set GOOGLE_APPLICATION_CREDENTIALS env var OR provide the path here.
VERTEX_AI_SERVICE_ACCOUNT_FILE=path/to/service-account.json
VERTEX_AI_CODING_MODEL=gemini-1.5-pro
VERTEX_AI_REASONING_MODEL=gemini-2.5-pro-preview-03-25

# --- OpenAI Settings ---
# For best outputs, use 'o3' for reasoning, or 'o4-mini' for a cheaper alternative.
OPENAI_API_KEY=
OPENAI_REASONING_MODEL=o4-mini
OPENAI_CODING_MODEL=gpt-4.1

# --- Azure OpenAI Settings ---
AZURE_OPENAI_KEY=your-api-key
AZURE_OPENAI_ENDPOINT=your-endpoint
AZURE_OPENAI_VERSION=2024-07-01-preview
AZURE_OPENAI_REASONING_MODEL=gpt-4
AZURE_OPENAI_CODING_MODEL=gpt-4-turbo

# --- Anthropic Settings ---
ANTHROPIC_API_KEY=sk-xxxx
ANTHROPIC_REASONING_MODEL=claude-3-sonnet-20240229
ANTHROPIC_CODING_MODEL=claude-3-sonnet-20240229

# --- OpenAI Compatible (for local LLMs like Ollama, etc.) ---
OPENAI_COMPATIBLE_REASONING_MODEL=llama3
OPENAI_COMPATIBLE_CODING_MODEL=llama3
# For Docker deployments, use host.docker.internal instead of localhost
# Local: http://localhost:11434/v1  |  Docker: http://host.docker.internal:11434/v1
OPENAI_COMPATIBLE_BASE_URL=http://localhost:11434/v1
# Most local LLMs don't require authentication - you can leave this blank or remove it
OPENAI_COMPATIBLE_TOKEN=ollama

# --- AWS Bedrock Settings ---
# AWS credentials can be provided via environment variables or IAM roles
AWS_ACCESS_KEY_ID=your-access-key
AWS_SECRET_ACCESS_KEY=your-secret-key
AWS_DEFAULT_REGION=us-east-1
# Bedrock model names - use full model identifiers
BEDROCK_REASONING_MODEL=anthropic.claude-3-sonnet-20240229-v1:0
BEDROCK_CODING_MODEL=anthropic.claude-3-sonnet-20240229-v1:0


# -------------------------------------------------------------------
#  5. Advanced Application Settings (Optional)
# -------------------------------------------------------------------

# Set to 'true' to use the reasoning agent for pre-processing queries.
USE_PRE_REASONING=true

# Set the application log level. Options: DEBUG, INFO
LOG_LEVEL=INFO

# Global LLM Settings (applies to all AI agents)
LLM_TEMPERATURE=0.7                  # Creativity vs consistency (0.0-1.0)
LLM_RETRIES=1                       # Number of retries for all agents
LLM_ENABLE_TOKEN_REPORTING=true     # Enable detailed token usage reporting

# Add custom headers for LLM calls, useful for security proxies. Can only be used with openai_compatible provider.
# Example: CUSTOM_HTTP_HEADERS={"x-api-key": "your-api-key"}
CUSTOM_HTTP_HEADERS={}

## TLS/SSL Settings
# Set to 'false' if you need to disable SSL verification (not recommended).
VERIFY_SSL=true
# To use a custom CA, place the file in src/backend/certs/ and provide the filename.
# SSL_CA_BUNDLE_FILENAME=custom-trust-certs.pem


# -------------------------------------------------------------------
#  6. Real-time Agent Mode Settings
# -------------------------------------------------------------------

# Note: Use standard LOG_LEVEL for console logging
# Optional: Use FILE_LOG_LEVEL for file-specific log level (defaults to DEBUG)
# FILE_LOG_LEVEL=DEBUG

# Max execution time for each step in the agent process (in seconds).
RLTIME_STEP_EXECUTION_TIMEOUT=300