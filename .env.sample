
OKTA_CLIENT_ORGURL=https://dev-2715.okta.com

###################
### IMPORTANT ####
###################
# this is the API token for the Okta API. You can generate this from the Okta Admin Console
# A read-only admin privilege is sufficient for this tool
# Make sure you increase the API limits, possbily to 100% for this token so the data fetch process is faster.
# https://help.okta.com/en-us/content/topics/security/api.htm?cshid=ext_API#Set
OKTA_API_TOKEN=


# Free / Developer tenants - If your API rate limit for the token is set to 50% , set the following to 6 . If rate limit is set to 100%, set this to 15
# For enterprise tenants - If your API rate limit for the token is set to 50% , set the following to 36 . If rate limit is set to 100%, set this to 75
OKTA_CONCURRENT_LIMIT=15

## Security Settings
COOKIE_SECURE=True
COOKIE_MAX_AGE_MINUTES=30
# keep this the same as COOKIE_MAX_AGE_MINUTES
JWT_ACCESS_TOKEN_EXPIRE_MINUTES=30

# Log Level can be set to DEBUG, INFO
LOG_LEVEL=INFO


#Can be set to true or false
USE_PRE_REASONING=true

# AI Provider Selection. IT can be one of the following values: vertex_ai, openai, azure_openai, openai_compatible, anthropic
AI_PROVIDER=openai_compatible

# Vertex AI Models (if using Vertex AI - gemini pro 2.5 is a hit or miss, 2.0 flash is working better)
VERTEX_AI_SERVICE_ACCOUNT_FILE=path/to/service-account.json   ## or set GOOGLE_APPLICATION_CREDENTIALS environment variable
VERTEX_AI_CODING_MODEL=gemini-2.0-flash
VERTEX_AI_REASONING_MODEL=gemini-2.0-flash


# OpenAI Models (if using OpenAI)
OPENAI_API_KEY=
OPENAI_REASONING_MODEL=gpt-4o
OPENAI_CODING_MODEL=gpt-4o

# Azure OpenAI Models (if using Azure)
AZURE_OPENAI_KEY=your-api-key
AZURE_OPENAI_ENDPOINT=your-endpoint
AZURE_OPENAI_VERSION=2024-07-01-preview
AZURE_OPENAI_REASONING_MODEL=gpt-4
AZURE_OPENAI_CODING_MODEL=gpt-4-turbo


#Anthropic Models (if using Anthropic)
ANTHROPIC_API_KEY=sk-xxxx
ANTHROPIC_REASONING_MODEL=claude-3-7-sonnet-latest
ANTHROPIC_CODING_MODEL=claude-3-7-sonnet-latest


#Pass custom hearders to make yor LLM call. This is useful for custom LLMs that require custom headers to be passed in the request or for LLM security proxies
CUSTOM_HTTP_HEADERS={"x-api-key": "your-api-key"}

# OpenAI Compatible Configuration
OPENAI_COMPATIBLE_REASONING_MODEL=accounts/fireworks/models/qwen2p5-72b-instruct
OPENAI_COMPATIBLE_CODING_MODEL=accounts/fireworks/models/qwen2p5-coder-32b-instruct
OPENAI_COMPATIBLE_BASE_URL=https://api.fireworks.ai/inference/v1
OPENAI_COMPATIBLE_TOKEN=


# OpenAI Compatible Configuration - Ollama
#OPENAI_COMPATIBLE_REASONING_MODEL=qwen2.5:14b
#OPENAI_COMPATIBLE_CODING_MODEL=qwen2.5-coder:32b
#OPENAI_COMPATIBLE_BASE_URL=http://localhost:11434/v1/
#OPENAI_COMPATIBLE_TOKEN=xxxxxx